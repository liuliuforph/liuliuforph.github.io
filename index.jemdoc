# jemdoc: menu{MENU}{index.html}, addcss{examples.css}, addcss{mktree.css}, nofooter
= Liu Liu

~~~
{}{img_left}{./photo-me.jpg}{photo}{200px}{200px}{}

Ph.D student \n
Member of the [http://wncg.org/  Wireless Networking and Communications Group] \n
Department of [http://www.ece.utexas.edu/ Electrical and Computer Engineering] \n
The University of Texas at Austin \n



~~~


~~~

== Contact

- Email: liuliuforph \[at\] gmail \[dot\] com \n
- [https://scholar.google.com/citations?user=hP8aSTAAAAAJ&hl=en Google Scholar Profile]
~~~



== About Me
I am a Ph.D student in the ECE department of The University of Texas at Austin.
I am fortunately advised by [http://users.ece.utexas.edu/~cmcaram/constantine_caramanis/Home.html Prof. Constantine Caramanis].
My research interests are in theoretical machine learning, optimization and high dimensional statistics.

Before moving to Austin, I received the B.E. degree from Department of Electronic Engineering, Tsinghua University, Beijing, China, in 2016.

\n

~~~
{}{raw}
<a name="research">
~~~

== Publications and Preprints
~~~
{}{raw}
<script type="text/javascript" language="JavaScript" src="mktree.js"></script>
<p class="jshide" id="jsonly"> 
<a href="#" onclick="expandTree('tree1'); return false;">Expand all abstracts</a>&nbsp;&nbsp;
<a href="#" onclick="collapseTree('tree1'); return false;">Collapse all abstracts</a></p>
~~~


{{<div class="mktree" id="tree1">}}

*High Dimensional Robust M-Estimation: Arbitrary Corruption and Heavy Tails *\n
*Liu Liu*, Tianyang Li, Constantine Caramanis.\n
ArXiv preprint arXiv:1901.08237, 2019. \[[http://arxiv.org/abs/1901.08237 Arxiv Link]\] \[[./1901.08237.pdf PDF]\] 
- *See abstract*
-- We consider the problem of sparsity-constrained $M$-estimation when both {\em explanatory and response} variables have heavy tails (bounded 4-th moments), or a fraction of arbitrary corruptions. We focus on the $k$-sparse, high-dimensional regime where the number of variables $d$ and the sample size $n$ are related through $n \sim k \log d$. We define a natural condition we call the Robust Descent Condition (RDC), and show that if a gradient estimator satisfies the RDC, then Robust Hard Thresholding (IHT using this gradient estimator), is guaranteed to obtain good statistical rates. The contribution of this paper is in showing that this RDC is a flexible enough concept to recover known results, and obtain new robustness results. Specifically, new results include: (a) For $k$-sparse high-dimensional linear- and logistic-regression with heavy tail (bounded 4-th moment) explanatory and response variables, a linear-time-computable median-of-means gradient estimator satisfies the RDC, and hence Robust Hard Thresholding is minimax optimal; (b) When instead of heavy tails we have $O(1/\sqrt{k}\log(nd))$-fraction of arbitrary corruptions in explanatory and response variables, a near linear-time computable trimmed gradient estimator satisfies the RDC, and hence Robust Hard Thresholding is minimax optimal.
We demonstrate the effectiveness of our approach in sparse linear, logistic regression, and sparse precision matrix estimation on synthetic and real-world US equities data.

\n

*High Dimensional Robust Sparse Regression*\n
*Liu Liu*, Yanyao Shen, Tianyang Li, Constantine Caramanis.\n
ArXiv preprint arXiv:1805.11643, 2018. \[[https://arxiv.org/abs/1805.11643  Arxiv Link]\] \[[./1805.11643.pdf  PDF]\] \[[./Simons_Lightning.pdf Slides]\]
- *See abstract*
-- We provide a novel -- and to the best of our knowledge, the first -- algorithm for high dimensional sparse regression with constant fraction of corruptions in explanatory and/or response variables. Our algorithm recovers the true sparse parameters with sub-linear sample complexity, in the presence of a constant fraction of arbitrary corruptions. Our main contribution is a robust variant of Iterative Hard Thresholding. Using this, we provide accurate estimators: when the covariance matrix in sparse regression is identity,  our error guarantee is near information-theoretically optimal. We then deal with robust sparse regression with unknown structured covariance matrix. We propose a filtering algorithm which consists of a novel randomized outlier removal technique for robust sparse mean estimation that may be of interest in its own right: the filtering algorithm is flexible enough to deal with unknown covariance. Also, it is orderwise more efficient computationally than the ellipsoid algorithm. Using sub-linear sample complexity, our algorithm achieves the best known (and first) error guarantee. 
We demonstrate the effectiveness on large-scale sparse regression problems with arbitrary corruptions.

\n

*Statistical inference using SGD*\n
Tianyang Li, *Liu Liu*, Anastasios Kyrillidis, Constantine Caramanis.\n
Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, 2018. \[[https://arxiv.org/abs/1705.07477  Arxiv Link]\] \[[https://arxiv.org/pdf/1705.07477.pdf  PDF]\]
- *See abstract*
-- We present a novel method for frequentist statistical inference in M-estimation problems, based on stochastic gradient descent (SGD) with a fixed step size: we demonstrate that the average of such SGD sequences can be used for statistical inference, after proper scaling. An intuitive analysis using the Ornstein-Uhlenbeck process suggests that such averages are asymptotically normal. From a practical perspective, our SGD-based inference procedure is a first order method, and is well-suited for large scale problems. To show its merits, we apply it to both synthetic and real datasets, and demonstrate that its accuracy is comparable to classical statistical methods, while requiring potentially far less computation.

\n


*Approximate Newton-based statistical inference using only stochastic gradients*\n
Tianyang Li, Anastasios Kyrillidis, *Liu Liu*, Constantine Caramanis.\n
ArXiv preprint arXiv:1805.08920, 2018. \[[https://arxiv.org/abs/1805.08920  Arxiv Link]\] \[[https://arxiv.org/pdf/1805.08920.pdf  PDF]\]
- *See abstract*
-- We present a novel inference framework for convex empirical risk minimization, using approximate stochastic Newton steps. The proposed algorithm is based on the notion of finite differences and allows the approximation of a Hessian-vector product from first-order information. In theory, our method efficiently computes the statistical error covariance in M-estimation, both for unregularized convex learning problems and high-dimensional LASSO regression, without using exact second order information, or resampling the entire data set. In practice, we demonstrate the effectiveness of our framework on large-scale machine learning problems, that go even beyond convexity: as a highlight, our work can be used to detect certain adversarial attacks on neural networks.


\n

\n
\n

